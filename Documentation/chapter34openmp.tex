%----------------------------------------------------------------------------
\section{OpenMP}
%----------------------------------------------------------------------------
	Elsõ lépésben az egyik legegyszerûbben elérhetõ és relatíve kevés kód írást igénylõ OpenMP felhasználásával igyekeztem a munkát szétosztani szálak között. Kiválóan alkalmazható olyan problémák esetén, ahol a párhuzamosítandó részek mérete futási idõben dõl el.

	Az OpenMP (Open Multi-Processing) egy specifikáció, kiegészítése a C és C++ nyelveknek, mely már igen széleskörû támogatottságot élvez a nagy hardver- és szoftvergyártók részérõl. 

	A kiegészítés elsõsorban fordító direktívákat definiál, melyek használatával kijelölhetõk a kódban azok a program részek, amelyeket szeretnénk, ha a fordító megpróbálna párhuzamosítani. Az úgynevezett \emph{fork-join} modellt valósítja meg az OpenMP, melyben a teljes program felosztható egymást követõ szekvenciális és párhuzamos részekre. A program sorosan végrehajtandó kóddal indul, majd elágazik egymástól független részekre és végül újra egyesül \cite{omp}.

	\begin{figure}[!ht]
	\centering
	\includegraphics[width=150mm, keepaspectratio]{figures/Implementation/Impl_OMP_ForkJoin.pdf}
	\caption{Fork-join modell}
	\label{fig:Impl_ForkJoin}
	\end{figure}

	Bizonyos helyzetekben elég néhány sort beírni egy teljesen szekvenciális programba és máris számottevõ növekedést lehet elérni. Mint minden párhuzamosítási technológiánál itt is figyelni kell a közös használatú változókra, amennyiben van ilyen. Bizonyos fordítók automatikusan szétosztják a feladatokat a processzormagok között.

	Az egyik legjobban párhuzamosítható programrész, az egymástól \emph{független adatokkal dolgozó ciklus}. A megoldás itt az jelenti, ha a ciklust feldaraboljuk, annyi kisebb ciklusra ahány szálat szeretnénk létrehozni. A keletkezett kisebb ciklusok végrehajtását pedig szétosztjuk a magok között:

	\begin{lstlisting}
// Eredeti ciklus
for(int i = 0; i < N ; i++) {
	c[i] = a[i] + b[i];
}

// 4 ciklusra felbontva
for(int i = 0; i < N/4 ; i++) {...}		 // 1. mag
for(int i = N/4; i < N/2 ; i++) {...}   // 2. mag
for(int i = N/2; i < 3*N/4 ; i++) {...} // 3. mag
for(int i = 3*N/4; i < N ; i++) {...}   // 4. mag
	\end{lstlisting}

	A magok közötti szétosztást a fenti módszerrel még nem tettük meg, viszont így látható, hogy milyen irányból közelíti meg a problémát az OpenMP. Az alábbi \verb+pragma+ direktívával kiegészített kód hatására a fordító párhuzamosítani fogja a közvetlenül utána következõ \verb+for+ ciklust. Minden szálhoz létrehoz egy ciklusváltozót lokálisan, a többi korábban deklarált változó $(N, a, b, c)$ viszont közös használatú. a \verb+num_threads(x)+ határozza meg a létrehozandó szálak számát. Amennyiben nincs megadva az alapértelmezett érték a maximum elérhetõ szálszám lesz. Esetemben ez a \tabref{szgspecifikacio} táblázat alapján 8.

	\begin{lstlisting}
#pragma omp parallel for num_threads(4)
for(int i = 0; i < N ; i++) {
	c[i] = a[i] + b[i];
}
	\end{lstlisting}
	
	A referencia algoritmusban bemutatott két \verb+for+ ciklus közül a külsõt érdemes párhuzamosítani. Ezzel minden mag különbözõ testnek fogja a gyorsulás értékeit frissíteni, s nem lesz szükséges semmiféle szinkronizáció vagy atomi mûvelet használata. A ciklusváltozó és a gyorsulásértékek ideiglenes tárolására létrehozott belül lett deklarálva, így minden szál számára lokális változók lesznek.

	Mivel fordításidejû párhuzamosításról van szó, annak megoldását, hogy ez a funkció ki-bekapcsolható legyen, némi redundancia segítségével értem el:
	
	\begin{lstlisting}
void NBodyAlgorithmCPUAllPairs::advance(std::vector<Body> &bodies) {
	...
	if (mp_properties->useOpenMP) {
	#pragma omp parallel for
		for (int i = 0; i < mp_properties->numBody; i++) {
			// Belsõ for ciklus, és a calculateAcceleration() metódus hívása
		}
	}
	else {
		for (int i = 0; i < mp_properties->numBody; i++) {
			// Belsõ for ciklus, és a calculateAcceleration() metódus hívása
		}
	}
	...
}
	\end{lstlisting}

	A két \verb+for+ ciklus közül csak az egyik elé került \verb+pragma+ direktíva, a másik teljesen szekvenciális maradt. A fordítás után a \verb+boolean+ típusú \verb+useOpenMP+ változó állításával lehet választani közülük. Hasonló megoldással az integráló rész \verb+for+ ciklusát is párhuzamosítottam.

	Egy másik remek OpenMP direktíva, a szintén ciklusok elé írható az \verb+unroll+. Ezt akkor érdemes használni, ha egy ciklusnak a magja kevés utasításból áll, vagyis a feltételvizsgálat és a hozzátartozó ugró utasítás számottevõ, összemérhetõ részt tesz ki a ,,hasznos'' kóddal. Ennek hatására a ciklus mag többször egymás alá másolódik a memóriában, és a ciklusváltozó nagyobb léptékû lesz.

	\begin{lstlisting}
#pragma unroll(4)
for(int i = 0; i < N ; i++) {
	c[i] = a[i] + b[i];
}
	\end{lstlisting}

	A fenti kód hatása ekvivalens lesz:

	\begin{lstlisting}
for(int i = 0; i < N ; i += 4) {
	c[i]   = a[i]   + b[i];
	c[i+1] = a[i+1] + b[i+1];
	c[i+2] = a[i+2] + b[i+2];
	c[i+3] = a[i+3] + b[i+3];
}
	\end{lstlisting}

	Ezzel a vezérlõ (control-flow) utasítások aránya csökkent, a ciklus mag utasításainak számához képest. A fordítók sokszor automatikusan élnek ezzel a megoldással. Ha $N$ kicsi elõfordulhat, hogy a ciklus teljes mértékben ki lesz bontva. Egyetlen hátránya az \verb+unroll+-nak, hogy a kód hosszabb lesz és több helyet foglal fordítás után.

	Látható, hogy a szimulátor számításigényesebb részeit mindösszesen két sor beírásával sikerült párhuzamosítani, a probléma jellegébõl adódóan. A független számításokat tartalmazó kódrészletek, nagyon hatékonyan párhuzamosíthatók OpenMP segítségével. A profilert eredménye látható a \figref{Impl_basic_vs_omp} ábrán, a négy magos CPU kihasználtsága jelentõsen megnövekedett és a futási idõ lerövidült.

	\begin{figure}[!ht]
	\centering
	\includegraphics[width=150mm, keepaspectratio]{figures/Performance/basic_vs_omp.png}
	\caption{$1024$ test, $1000$ iteráció szimuláció futási eredménye OpenMP nélkül és OpenMP-vel}
	\label{fig:Impl_basic_vs_omp}
	\end{figure}

	Az OpenMP használatának másik elõnye, hogy a soron következõ SSE és AVX megoldásokkal együtt használható, ugyanis elõbbi a processzormagok közötti, a másik kettõ pedig magokon belüli párhuzamosítás.
