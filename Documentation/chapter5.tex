%----------------------------------------------------------------------------
\chapter{Eredmények kiértékelése}
%----------------------------------------------------------------------------
	A szimulációt lefuttattam, minden felhasznált technológiával, illetve kombinációikkal, azonos paraméterezéssel. Az eredményeket egy Excel táblázatban hasonlítottam össze és jelenítettem meg. Összesen 7 különbözõ testszámmal végeztem el a teszteket: 8 ,256, 512, 1024, 2048, 4096, 16384. Mindegyik teszt összesen 20 napnyi idõintervallumot szimulál, 1 napos lépésközzel, tehát összesen 20 szimulációs ,,tick'' történt. A mérések tisztán csak a ,,hasznos'' iterációkat tartalmazzák, a program kezdeti inicializációs fázisát nem, mely nagy testszám esetén sokáig is eltarthat.
	
	Az ábrákat méretük miatt a függelékben helyeztem el. Az ábrákon látható értékeket 20 futási eredménynek (szimulációnak) az átlaga adta ki ezzel eltüntetve, az ideiglenes számítógép terheltségébõl adódó ingadozásokat. Az \figref{Perf_GFLOPS} ábrán látható az egyes implementációk által másodpercenként elvégzett milliárd lebegõpontos számítások (GFLOPS) száma. Egy talán megfoghatóbb jellemzés lehet a másodpercenként elvégzett iterációk száma, mely lényegében tekinthetõ FPS-nek (Frames Per Second). Ez látható a \figref{Perf_FPS} ábrán 4096 test esetére.
	
	A futási idõket a jobb ábrázolhatóság érdekében logaritmikus skálán jelenítettem meg (\figref{Perf_FUTAS} ábra).
	
	MATLAB prototípus futási eredményeit nem ábrázoltam, mert szkriptnyelvrõl lévén szó nagyságrendekkel kisebb teljesítményt produkált, így lényegében nem is látszódna a grafikonon.

	%----------------------------------------------------------------------------
	\section{CPU}
	%----------------------------------------------------------------------------
		A referenciamodellem gyorsan elérte a futási teljesítményének határát. A 8 test mozgásának szimulálása során a referenciamodell teljesített a legjobban.
		
		SSE és AVX közti probléma is jól látható, sajnos nem sikerült elérnem, hogy az AVX papírforma szerint túlteljesítse elõdjét.
		
		Bár elvileg 8 szál futtatására képes a processzorom, de valójában a 4 magon megvalósított Intel szálkezelõ megoldás (HyperThreading) -- mely kontextusváltást gyorsítja -- idõosztásos alapon mûködik. A sebességnövekedés OpenMP használatával így körülbelül négyszeres lehetne maximum. Az OpenMP használatával nyert eredmények, a megvalósítás során bemutatott \figref{Impl_basic_vs_omp} ábrán láthatók.
		
		Feltüntettem az ideiglenes megoldások, próbálkozások nyújtotta eredményeket, melyeket az AVX felhasználást bemutató \sectref{Impl_AVXSect} alfejezetben részleteztem. Az eredményekbõl leolvasható, hogy a dinamikus tömböket alkalmazó \verb+NBodySystemFlat+ kétszeres sebességnövekedést ért el a SSE-t és OpenMP-t együttesen felhasználó eredeti megoldáshoz képest.
		
		Összegezve elmondható, hogy alacsony testszámú szimulációs esetben mindenképp egy CPU-s implementációt érdemes használni.
	%----------------------------------------------------------------------------
	\section{GPU}
	%----------------------------------------------------------------------------
		%----------------------------------------------------------------------------
		\subsection{Elméleti teljesítmény}
		%----------------------------------------------------------------------------
		Az eredményekbõl látható, hogy alacsony testszám (szálszám) mellett a GPU nem teljesített kiemelkedõen. A rendelkezésemre álló NVIDIA GeForce 525M elméleti teljesítményét (Theorethical Peak Performance) a következõ összefüggés alapján kaphatjuk meg:
		
		\begin{align}
			P_T = \frac{n \cdot f \cdot 2}{1000} = \frac{96 \cdot 600 \cdot 2}{1000} = 115,2~GFLOPS
		\end{align}
		
		Ahol $n$ jelöli a CUDA magok számát, $f$ az órajelük frekvenciáját (MHz). A $2$ szorzó az órajel ciklusonként végrehajtott 2 darab utasítás miatt jelenik meg.
		
		\begin{align}
			P_T = \frac{n \cdot f \cdot 2}{1000}
		\end{align}
		
		32768 volt a legtöbb test, amivel a szimulációt lefuttattam, aminél 49 GFLOPS számítási teljesítményt sikerült elérnem. A elméleti korlátnak ez sajnos csak közel a fele. A profiler eredményekbõl látszik, hogy elég sok a control-flow jellegû utasítás a kernelben, melyre a generált kód megismerése és elemzése adhatna magyarázatot.

		%----------------------------------------------------------------------------
		\subsection{Visual Profiler eredményei}
		%----------------------------------------------------------------------------
		% Profiler eredmények bemutatása
		A GPU-s implementáció futását az NVIDIA Visual Profiler programjával elemeztem ki. A programom idõdiagrammját tartalmazza az \figref{Perf_visual_prof_1} és az \figref{Perf_visual_prof_2} ábra kettõ, egymást követõ részletben.
		
		Az elsõ ábrán történik az inicializáció, mely során az eszköz memóriájában megtörténik az allokáció a \verb+cudaMalloc+ függvény segítségével.
		
		A második részben látható a kezdeti adatok átmásolása az eszköz memóriájába majd a kernel futása összesen 20-szor. Minden iteráció után megtörténik az meghatározott adatok visszamásolása a rendszermemóriába. A legvégén pedig az alaphelyzetbe állító függvény a \verb+cudaDeviceReset+ fut le.
		
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/Performance/visual_prof_1.png}
			\caption{Inicializáció és allokáció}
			\label{fig:Perf_visual_prof_1}
		\end{figure}
		
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/Performance/visual_prof_2.png}
			\caption{Kernel futása és a szimuláció befejezése}
			\label{fig:Perf_visual_prof_2}
		\end{figure}
		
		A profiler szerint a kernelem legnagyobb korlátját a megfelelõ számítási kapacitás hiánya okozza\footnote{Másik gyakori korlátozó tényezõ a kernelek esetén a memóriasávszélesség.}.
		
		\begin{quote}
			For device ,,GeForce GT 525M'' the kernel's memory utilization is significantly lower than the its compute utilization. These utilization levels indicate that the performance of the kernel is most likely being limited by computation on the SMs.
		\end{quote}
		Az analízis során kiderült, hogy a vezérlõ (control-flow) jellegû utasítások aránya viszonylag magas, a ,,hasznos'' aritmetikai utasításokhoz képest. Az arányokat az \figref{Perf_utilization} ábra jelzi.
		
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/Performance/utilization.png}
			\caption{Utasítások eloszlása}
			\label{fig:Perf_utilization}
		\end{figure}

		A megfelelõ GPU kihasználtságot (occupancy) jelenleg a regiszter felhasználás korlátozza. Minden szál 34 regisztert használ fel, így 128 szálból álló TB esetén 4352 regisztert foglal le egy TB. Így összesen 7 TB fér el egy SM-en a maximális 8-ból. Az egyszerre aktív warpok száma azonban lényegesen alacsonyabb, mindössze $23/48$, ugyanis a TB-k mérete relatíve kicsi: [128, 1, 1]. Ez a 128 szállal rendelkezõ kernel produkálta a legjobb eredményeket, azonban az elért GPU kihasználtság így mindössze csak 48\%, ahogy az \figref{Perf_kernel_properties} ábra is mutatja.
		
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=100mm, keepaspectratio]{figures/Performance/kernel_properties.png}
			\caption{A kernel tulajdonságai}
			\label{fig:Perf_kernel_properties}
		\end{figure}